g <- g + geom_abline(intercept=z[d],slope=slope)
#test data set and classifier
g
trajectorySlope <- (-1)/(Z_history[,2]/Z_history[,1])
trajectoryOffset <- Z_history[,3]
trajectory <- melt(cbind(trajectorySlope,trajectoryOffset))
g2 <- ggplot(melt(S2D),aes(S2D[,1],S2D[,2],color=factor(y)))+geom_point()
g2 <- g2 + geom_abline(aes(intercept=trajectoryOffset,slope=trajectorySlope),data=trajectory)
#Yubo Han yh2635
#install.packages("ggplot2")
#install.packages("reshape")
#S is a n by d+1 matrix
#z=(v1,...vd,-c) column vector
#return a n*1 column vector of class labels
#Part I
classify <- function(S,z){
sign <- S%*%z
sign[sign>=0] <- 1
sign[sign<0] <- -1
return(sign)
}
#help function
#make the first part of z, i.e. vh, a unit vector
#leave offset c intact
makeUnit <- function(z){
d <- length(z)
vh <- z[1:d-1]
vh <- vh/(sum(vh^2))^0.5
z <- c(vh,z[d])
return(z)
}
#Part II
#S is a n by d+1 matrix
#y are true labels, n*1 column vector
perceptrain <- function(S, y){
Z_history <- matrix(,nrow=0,ncol=ncol(S))
i <- 0
#randomly initialize z as a column vector
z <- rep(1,times=ncol(S))
z <- makeUnit(z)
#infinite loop until 0 error
while(TRUE){
i <- i+1
classMistake <- classify(S,z)-y
classMistake[classMistake!=0] <- 1
if (sum(classMistake)==0){
break
}
adjust <- S*c(classMistake)*(-y)
adjust <- apply(adjust,2,sum)
#adjust is d+1 * 1
z <- z-(1/i)*adjust
z <- makeUnit(z)
Z_history <- rbind(Z_history,t(z))
}
return(list(z=z,Z_history=Z_history))
}
#Part III
#test
z <- makeUnit(runif(3,-1,1))
print(z)
list <- fakedata(z,100)
S=list[["S"]]
y=list[["y"]]
list2 <- perceptrain(S,y)
testList <- fakedata(z,100)
check <- classify(testList[["S"]],list2[["z"]])
print(check==testList[["y"]])
# print all "TRUE" (or mostly TRUE's, since this is
# a new dataset), correct implementation!
#convert data and vectors into their 2D representation
d <- length(z)
z2D <- z[1:d-1]
S2D <- S[,1:d-1]
Z_history <- list2[["Z_history"]]
test2D <-testList[["S"]][,1:d-1]
testResult <- testList[["y"]]
slope <- (-1)/(z2D[2]/z2D[1])
#Part IV
library("ggplot2")
library("reshape2")
g <- ggplot(melt(test2D),aes(test2D[,1],test2D[,2],color=factor(testResult)))+geom_point()
g <- g + geom_abline(intercept=z[d],slope=slope)
#test data set and classifier
g
trajectorySlope <- (-1)/(Z_history[,2]/Z_history[,1])
trajectoryOffset <- Z_history[,3]
trajectory <- melt(cbind(trajectorySlope,trajectoryOffset))
g2 <- ggplot(melt(S2D),aes(S2D[,1],S2D[,2],color=factor(y)))+geom_point()
g2 <- g2 + geom_abline(aes(intercept=trajectoryOffset,slope=trajectorySlope),data=trajectory)
#Yubo Han yh2635
#install.packages("ggplot2")
#install.packages("reshape")
#S is a n by d+1 matrix
#z=(v1,...vd,-c) column vector
#return a n*1 column vector of class labels
#Part I
classify <- function(S,z){
sign <- S%*%z
sign[sign>=0] <- 1
sign[sign<0] <- -1
return(sign)
}
#help function
#make the first part of z, i.e. vh, a unit vector
#leave offset c intact
makeUnit <- function(z){
d <- length(z)
vh <- z[1:d-1]
vh <- vh/(sum(vh^2))^0.5
z <- c(vh,z[d])
return(z)
}
#Part II
#S is a n by d+1 matrix
#y are true labels, n*1 column vector
perceptrain <- function(S, y){
Z_history <- matrix(,nrow=0,ncol=ncol(S))
i <- 0
#randomly initialize z as a column vector
z <- rep(1,times=ncol(S))
z <- makeUnit(z)
#infinite loop until 0 error
while(TRUE){
i <- i+1
classMistake <- classify(S,z)-y
classMistake[classMistake!=0] <- 1
if (sum(classMistake)==0){
break
}
adjust <- S*c(classMistake)*(-y)
adjust <- apply(adjust,2,sum)
#adjust is d+1 * 1
z <- z-(1/i)*adjust
z <- makeUnit(z)
Z_history <- rbind(Z_history,t(z))
}
return(list(z=z,Z_history=Z_history))
}
#Part III
#test
z <- makeUnit(runif(3,-1,1))
print(z)
list <- fakedata(z,100)
S=list[["S"]]
y=list[["y"]]
list2 <- perceptrain(S,y)
testList <- fakedata(z,100)
check <- classify(testList[["S"]],list2[["z"]])
print(check==testList[["y"]])
# print all "TRUE" (or mostly TRUE's, since this is
# a new dataset), correct implementation!
#convert data and vectors into their 2D representation
d <- length(z)
z2D <- z[1:d-1]
S2D <- S[,1:d-1]
Z_history <- list2[["Z_history"]]
test2D <-testList[["S"]][,1:d-1]
testResult <- testList[["y"]]
slope <- (-1)/(z2D[2]/z2D[1])
#Part IV
library("ggplot2")
library("reshape2")
g <- ggplot(melt(test2D),aes(test2D[,1],test2D[,2],color=factor(testResult)))+geom_point()
g <- g + geom_abline(intercept=z[d],slope=slope)
#test data set and classifier
g
trajectorySlope <- (-1)/(Z_history[,2]/Z_history[,1])
trajectoryOffset <- Z_history[,3]
trajectory <- melt(cbind(trajectorySlope,trajectoryOffset))
g2 <- ggplot(melt(S2D),aes(S2D[,1],S2D[,2],color=factor(y)))+geom_point()
g2 <- g2 + geom_abline(aes(intercept=trajectoryOffset,slope=trajectorySlope),data=trajectory)
ggsave("test_and_classifier.png",g)
g2
ggsave("training_and_trajectory.png",g2)
library(e1071)
library(rpart)
uspsdata = read.table("uspsdata.txt")
uspscl = read.table("uspscl.txt")
index <- nrow(uspsdata)
testIndex <- sample(index, floor(index*0.2))
testSet = uspsdata[testIndex,]
testLabel = factor(uspscl[testIndex,])
trainSet = uspsdata[-testIndex,]
trainLabel = factor(uspscl[-testIndex,])
sequence <- 10^seq(-5,5,0.1)
linearAccuracy <- c()
#linear model
for (i in sequence){
linearModel <- svm(x=trainSet, y=trainLabel,cross=5, kernel="linear",cost=i)
linearAccuracy <- c(linearAccuracy,linearModel$tot.accuracy)
}
linearMiss <- 100 - linearAccuracy
library(ggplot2)
plot1 <- qplot(log(c(sequence)),linearMiss,xlab="log cost",ylab="mistake rate",main="Linear Mistake")+geom_line()
plot1
ggsave("cv_misclassification_margin_linear.png",plot1)
#train the optimal
index <- which.max(linearAccuracy)
#may be more than one; just choose the first
optimalcostLinear <- sequence[index[1]]
optimalLinear <- svm(x=trainSet,y=trainLabel,cross=5,kernel="linear",cost=optimalcostLinear)
linearPred <- predict(optimalLinear, testSet)
length(linearPred[linearPred==testLabel])
length(linearPred)
linearTestMiss <- 1-length(linearPred[linearPred==testLabel])/length(testLabel)
linearTestMiss
optimalcostLinear
sequence2 <- 10^seq(-5,5,0.5)
RBFAccuracy <- matrix(0,length(sequence2),length(sequence2))
for (i in 1:length(sequence2)){
a <- sequence2[i]
for (j in 1:length(sequence2)){
b <- sequence2[j]
RBFModel <- svm(x=trainSet, y=trainLabel,cross=5,kernel="radial",cost=a,gamma=b)
RBFPred <- predict(RBFModel, testSet)
RBFAccuracy[i,j] <- RBFModel$tot.accuracy
}
}
RBFMiss <- 100 - RBFAccuracy
library(scatterplot3d)
x <- rep(sequence2,each=length(sequence2))
y <- rep(sequence2,length(sequence2))
z <- as.vector(t(RBFMiss))
plot2 <- scatterplot3d(log(x),log(y),z, main="RBF Mistake Rate", xlab="cost",ylab="gamma",zlab="mistake rate")
plot2
#train the RBF optimal
index2 <- which(RBFAccuracy == max(RBFAccuracy), arr.ind = TRUE)
#may be more than one combinations, just choose the first combination here
optimalcost <- sequence2[index2[1,1]]
optimalgamma <- sequence2[index2[1,2]]
optimalRBF <- svm(x=trainSet,y=trainLabel,cross=5,cost=optimalcost,gamma=optimalgamma)
RBFPred <- predict(optimalRBF, testSet)
RBFTestMiss <- 1-length(RBFPred[RBFPred==testLabel])/length(testLabel)
optimalcost
optimalgamma
RBFTestMiss
#Yubo Han
#yh2635
#HW 02
#STAT W4400
#install.packages("e1071")
library(e1071)
library(rpart)
uspsdata = read.table("uspsdata.txt")
uspscl = read.table("uspscl.txt")
index <- nrow(uspsdata)
testIndex <- sample(index, floor(index*0.2))
testSet = uspsdata[testIndex,]
testLabel = factor(uspscl[testIndex,])
trainSet = uspsdata[-testIndex,]
trainLabel = factor(uspscl[-testIndex,])
sequence <- 10^seq(-5,5,0.1)
linearAccuracy <- c()
#linear model
for (i in sequence){
linearModel <- svm(x=trainSet, y=trainLabel,cross=5, kernel="linear",cost=i)
linearAccuracy <- c(linearAccuracy,linearModel$tot.accuracy)
}
linearMiss <- 100 - linearAccuracy
library(ggplot2)
plot1 <- qplot(log(c(sequence)),linearMiss,xlab="log cost",ylab="mistake rate",main="Linear Mistake")+geom_line()
#train the optimal
index <- which.max(linearAccuracy)
#may be more than one; just choose the first
optimalcostLinear <- sequence[index[1]]
optimalLinear <- svm(x=trainSet,y=trainLabel,cross=5,kernel="linear",cost=optimalcostLinear)
linearPred <- predict(optimalLinear, testSet)
linearTestMiss <- 1-length(linearPred[linearPred==testLabel])/length(testLabel)
######REPORT
######test set misclassification rate:0.025
######parameter cost:0.0005011872
#RBF model
sequence2 <- 10^seq(-5,5,0.5)
RBFAccuracy <- matrix(0,length(sequence2),length(sequence2))
for (i in 1:length(sequence2)){
a <- sequence2[i]
for (j in 1:length(sequence2)){
b <- sequence2[j]
RBFModel <- svm(x=trainSet, y=trainLabel,cross=5,kernel="radial",cost=a,gamma=b)
RBFPred <- predict(RBFModel, testSet)
RBFAccuracy[i,j] <- RBFModel$tot.accuracy
}
}
RBFMiss <- 100 - RBFAccuracy
library(scatterplot3d)
x <- rep(sequence2,each=length(sequence2))
y <- rep(sequence2,length(sequence2))
z <- as.vector(t(RBFMiss))
plot2 <- scatterplot3d(log(x),log(y),z, main="RBF Mistake Rate", xlab="cost",ylab="gamma",zlab="mistake rate")
#train the RBF optimal
index2 <- which(RBFAccuracy == max(RBFAccuracy), arr.ind = TRUE)
#may be more than one combinations, just choose the first combination here
optimalcost <- sequence2[index2[1,1]]
optimalgamma <- sequence2[index2[1,2]]
optimalRBF <- svm(x=trainSet,y=trainLabel,cross=5,cost=optimalcost,gamma=optimalgamma)
RBFPred <- predict(optimalRBF, testSet)
RBFTestMiss <- 1-length(RBFPred[RBFPred==testLabel])/length(testLabel)
######REPORT
######test set misclassification rate:0.05
######parameter cost:100 gamma:10^(-5)
#####Comparison: These two methods are comparable in terms of misclassification rate (linear even slightly better)
#####So linear SVM is more optimal in that it is faster
#####Probably because the number of features is so large (256) that it does not do
#####much help to map the data to an even higher dimension space
plot1
#Yubo Han
#yh2635
#HW 02
#STAT W4400
#install.packages("ggplot2")
#install.packages("reshape")
#S is a n by d+1 matrix
#z=(v1,...vd,-c) column vector
#return a n*1 column vector of class labels
#Part I
classify <- function(S,z){
sign <- S%*%z
sign[sign>=0] <- 1
sign[sign<0] <- -1
return(sign)
}
#help function
#make the first part of z, i.e. vh, a unit vector
#leave offset c intact
makeUnit <- function(z){
d <- length(z)
vh <- z[1:d-1]
vh <- vh/(sum(vh^2))^0.5
z <- c(vh,z[d])
return(z)
}
#Part II
#S is a n by d+1 matrix
#y are true labels, n*1 column vector
perceptrain <- function(S, y){
Z_history <- matrix(,nrow=0,ncol=ncol(S))
i <- 0
#randomly initialize z as a column vector
z <- rep(1,times=ncol(S))
z <- makeUnit(z)
#infinite loop until 0 error
while(TRUE){
i <- i+1
classMistake <- classify(S,z)-y
classMistake[classMistake!=0] <- 1
if (sum(classMistake)==0){
break
}
adjust <- S*c(classMistake)*(-y)
adjust <- apply(adjust,2,sum)
#adjust is d+1 * 1
z <- z-(1/i)*adjust
z <- makeUnit(z)
Z_history <- rbind(Z_history,t(z))
}
return(list(z=z,Z_history=Z_history))
}
#Part III
#test
z <- makeUnit(runif(3,-1,1))
print(z)
list <- fakedata(z,100)
S=list[["S"]]
y=list[["y"]]
list2 <- perceptrain(S,y)
testList <- fakedata(z,100)
check <- classify(testList[["S"]],list2[["z"]])
print(check==testList[["y"]])
# print all "TRUE" (or mostly TRUE's, since this is
# a new dataset), correct implementation!
#convert data and vectors into their 2D representation
d <- length(z)
z2D <- z[1:d-1]
S2D <- S[,1:d-1]
Z_history <- list2[["Z_history"]]
test2D <-testList[["S"]][,1:d-1]
testResult <- testList[["y"]]
slope <- (-1)/(z2D[2]/z2D[1])
#Part IV
library("ggplot2")
library("reshape2")
g <- ggplot(melt(test2D),aes(test2D[,1],test2D[,2],color=factor(testResult)))+geom_point()
g <- g + geom_abline(intercept=z[d],slope=slope)
#test data set and classifier
g
trajectorySlope <- (-1)/(Z_history[,2]/Z_history[,1])
trajectoryOffset <- Z_history[,3]
trajectory <- melt(cbind(trajectorySlope,trajectoryOffset))
g2 <- ggplot(melt(S2D),aes(S2D[,1],S2D[,2],color=factor(y)))+geom_point()
g2 <- g2 + geom_abline(aes(intercept=trajectoryOffset,slope=trajectorySlope),data=trajectory)
#Inputs
#w:  w[1:d] is the normal vector of a hyperplane,
#    w[d+1] = -c is the negative offset parameter.
#n: sample size
#Outputs
#S: n by (d+1) sample matrix with last col 1
#y: vector of the associated class labels
fakedata <- function(w, n){
if(! require(MASS))
{
install.packages("MASS")
}
if(! require(mvtnorm))
{
install.packages("mvtnorm")
}
require(MASS)
require(mvtnorm)
# obtain dimension
d <- length(w)-1
# compute the offset vector and a Basis consisting of w and its nullspace
offset <- -w[length(w)] * w[1:d] / sum(w[1:d]^2)
Basis <- cbind(Null(w[1:d]), w[1:d])
# Create samples, correct for offset, and extend
# rmvnorm(n,mean,sigme) ~ generate n samples from N(0,I) distribution
S <- rmvnorm(n, mean=rep(0,d),sigma = diag(1,d)) %*%  t(Basis)
S <- S + matrix(rep(offset,n),n,d,byrow=T)
S <- cbind(S,1)
# compute the class assignments
y <- as.vector(sign(S %*% w))
# add corrective factors to points that lie on the hyperplane.
S[y==0,1:d] <- S[y==0,1:d] + runif(1,-0.5,0.5)*10^(-4)
y = as.vector(sign(S %*% w))
return(list(S=S, y=y))
} # end function fakedata
#Yubo Han
#yh2635
#HW 02
#STAT W4400
#install.packages("ggplot2")
#install.packages("reshape")
#S is a n by d+1 matrix
#z=(v1,...vd,-c) column vector
#return a n*1 column vector of class labels
#Part I
classify <- function(S,z){
sign <- S%*%z
sign[sign>=0] <- 1
sign[sign<0] <- -1
return(sign)
}
#help function
#make the first part of z, i.e. vh, a unit vector
#leave offset c intact
makeUnit <- function(z){
d <- length(z)
vh <- z[1:d-1]
vh <- vh/(sum(vh^2))^0.5
z <- c(vh,z[d])
return(z)
}
#Part II
#S is a n by d+1 matrix
#y are true labels, n*1 column vector
perceptrain <- function(S, y){
Z_history <- matrix(,nrow=0,ncol=ncol(S))
i <- 0
#randomly initialize z as a column vector
z <- rep(1,times=ncol(S))
z <- makeUnit(z)
#infinite loop until 0 error
while(TRUE){
i <- i+1
classMistake <- classify(S,z)-y
classMistake[classMistake!=0] <- 1
if (sum(classMistake)==0){
break
}
adjust <- S*c(classMistake)*(-y)
adjust <- apply(adjust,2,sum)
#adjust is d+1 * 1
z <- z-(1/i)*adjust
z <- makeUnit(z)
Z_history <- rbind(Z_history,t(z))
}
return(list(z=z,Z_history=Z_history))
}
#Part III
#test
z <- makeUnit(runif(3,-1,1))
print(z)
list <- fakedata(z,100)
S=list[["S"]]
y=list[["y"]]
list2 <- perceptrain(S,y)
testList <- fakedata(z,100)
check <- classify(testList[["S"]],list2[["z"]])
print(check==testList[["y"]])
# print all "TRUE" (or mostly TRUE's, since this is
# a new dataset), correct implementation!
#convert data and vectors into their 2D representation
d <- length(z)
z2D <- z[1:d-1]
S2D <- S[,1:d-1]
Z_history <- list2[["Z_history"]]
test2D <-testList[["S"]][,1:d-1]
testResult <- testList[["y"]]
slope <- (-1)/(z2D[2]/z2D[1])
#Part IV
library("ggplot2")
library("reshape2")
g <- ggplot(melt(test2D),aes(test2D[,1],test2D[,2],color=factor(testResult)))+geom_point()
g <- g + geom_abline(intercept=z[d],slope=slope)
#test data set and classifier
g
trajectorySlope <- (-1)/(Z_history[,2]/Z_history[,1])
trajectoryOffset <- Z_history[,3]
trajectory <- melt(cbind(trajectorySlope,trajectoryOffset))
g2 <- ggplot(melt(S2D),aes(S2D[,1],S2D[,2],color=factor(y)))+geom_point()
g2 <- g2 + geom_abline(aes(intercept=trajectoryOffset,slope=trajectorySlope),data=trajectory)
g2
